{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GenFL - Posterior from a random Prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows an example of how to reproduce some results from the paper `Federated Learning with Nonvacuous Generalisation Bounds`.\n",
    "Specifically the ones from Table 1 : no KL penalty - 100 clients - Random prior - $f_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce a novel strategy to train randomised predictors in federated learning, where each node of the network aims at preserving its privacy by releasing a local predictor but keeping secret its training dataset with respect to the other nodes. We then build a global randomised predictor which inherits the properties of the local private predictors in the sense of a PAC-Bayesian generalisation bound. We consider the synchronous case where all nodes share the same training objective (derived from a generalisation bound), and the asynchronous case where each node may have its own personalised training objective. We show through a series of numerical experiments that our approach achieves a comparable predictive performance to that of the batch approach where all datasets are shared across nodes. More over the predictors are supported by numerically nonvacuous generalisation bounds while preserving privacy for each node. We explicitly compute the increment on predictive performance and generalisation bounds between batch and federated settings, highlighting the price to pay to preserve privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Startup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is meant to be used as a bash CLI. For pedagogic reasons, let's see it through a notebook file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/volatile/home/pj273170/Code/PAC_Bayes\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the code. We will produce a posterior with a config file in `./conf/scenario/perez_posterior_rand.yaml` using the default configuration `./conf/genfl_posterior.yaml`.\n",
    "\n",
    "You can see in the configuration file, that the neural network is a `ProbNNet4lPerez` than you can find `./core/model.py`. It's a stochastic neural network (SNN), we will train it to get a Posterior from a -random- prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fast computation, we will use `dryrun=True` (almost all loops once). Otherwise, with `dryrun=False`, it takes several hours (and we don't get the result from the paper, see below for explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "python GenFL_posterior.py \\\n",
    "    +scenario=perez_posterior_rand \\\n",
    "    dryrun=True \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always a directory has been created at `./outputs/GenFL_Posterior/[Date]/[Time]/+scenario=perez_posterior_rand`. Check the log file to verify that everything has gone right.  \n",
    "You can check the `/metrics` folder which has a lot more files than in the prior scenario.\n",
    "\n",
    "To summarize briefly all the metrics:\n",
    "- `[metric_name]_Metrics_tmp.png` is the name format\n",
    "- `[metric_name]_c_Metrics_tmp.png`, \"c\" stands for clients, which are some \"local metrics\" aggregated metrics computed only in the `evaluate` method in `./core/client.py`. (For instance, risk_01 is the generalisation bound for all the client, whereas risk_01_c is the aggregation of all local generalisation bounds of each client)\n",
    "- `[metric_name]_[ens/mean/stch]_Metrics_tmp.png` are the metrics from the [`testEnsemble`, `testPosteriorMean`, `testStochastic`] functions in `./core/train.py` (mostly from (centralized) `evaluate` method from `./core/strategy.py`)\n",
    "\n",
    "\n",
    "Metrics found in the paper, by metric names :\n",
    "- `accuracy_stch` : is the \"Test Err.\" column from Table 1.\n",
    "- `risk_01` : is the \"Bound\" column from Table 1.\n",
    "- `kl_n` : is the \"KL/m\" column from Table 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example `risk_01_Metrics_tmp.png`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
